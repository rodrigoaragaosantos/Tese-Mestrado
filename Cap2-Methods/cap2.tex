This chapter gives the information needed to understand the proposed methods. The chapter starts by describing the SAR systems used for image acquisitions. Then, the datasets used to assess the proposed methods are presented. Finally, the problems tackled with each dataset are described. The Amazon Rainforest dataset is considered for the problem of deforestation mapping and detection. For the CARABAS dataset, the problem that will be tackled is the change target detection problem. The chapter also introduces the methods and techniques used as references, specifically machine learning techniques such as random forests, neural networks, convolutional neural networks, and feature texture extraction.

\section{Materials}
This section describes satellite systems used for data acquisition: The Tandem-X satellite, DLR, the Sentinel-1 satellite, ESA, and the CARABAS-II system, FOI. It will also detail the test areas chosen to perform the study.

For the deforestation detection problem, it was chosen to use data from the Amazon Rainforest, focusing specifically on the Rondônia state area. Rondônia is a state in northern Brazil that is topographically composed mainly of lands and plateaus with low altitudes. The climate in Rondônia is called "humid equatorial," which means that the temperature variation throughout the year is minimal (something that is very desirable when working with SAR acquisitions since it is preferable to have as little variation as possible between acquisitions) \cite{rondoniaGeography}. The pluviometric indexes in the state can reach up to 2100 mm per year, with most of the rains happening between May and September - something that one must know to better select and compare data since the rainy season can affect the image acquisition, so one must always use images from the same season when comparing acquisitions.

One of the goals of this thesis is to improve algorithms for deforestation mapping and detection. Therefore, Rondônia was a natural choice for a study area since it is the state with the most deforestation in the Amazon rainforest. The Rondônia state has lost over 31\% of its forests, and most of the remaining areas are degraded. For comparison, Acre, the state which borders Rondônia on the west, has 91\% of its original forest cover, and a greater part of it is still intact \cite{rondoniaDeforestation} The deforestation in the Rondônia state can be easily seen with optical satellite data acquired with Google Earth. Deforestation follows a fairly predictable pattern, as seen in Figure \ref{fig:fishbone}. The pattern of deforestation is known as a fishbone pattern due to its similarity with a fishbone skeleton. This pattern arises because deforesting is usually done by penetrating the forest and then deforesting along the edges of the road.  Due to recent fires that happened in the year of 2019 in the Amazon Rainforest, there
is much concern about studying and monitoring the deforestation that happens in that
area, and considering that the Rondônia state is the area in which the deforestation is
most critical, it was a natural choice of area to study for this thesis.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Cap2-Methods/fishbone.png}
    \caption{Fishbone pattern of deforestation}
    \label{fig:fishbone}
\end{figure}


Another approach that we can take to solve the problem of monitoring deforestation is anticipating when deforestation will take place instead of monitoring it after it has happened. One strategy that one can take is trying to detect vehicles used for deforestation that might be concealed under tree foliage. There are many situations where there is a need for monitoring vehicles concealed by foliage over a large area. For example, for military applications, it might be necessary to monitor the positioning of enemy vehicles trespassing in a restricted location. For environmental purposes, it might be needed to pinpoint the position of vehicles used for illegal deforestation in overprotected forests. If SARs are to be used for such purposes, it is ideal that low-frequency systems are chosen since those provide a wide surveillance area and good foliage penetration capabilities. At low frequency (VHF-band), the main backscatter from the target area is due to large scatters, e.g., tree canopy, houses, vehicles, and other human-made objects, which will appear as very bright objects in the image.

The challenge related to change detection is associated with the trade-off between having high accuracy detection and a high false alarm rate. Typically, trying to increase the overall accuracy of detection algorithms will incur a high number of false alarms. Therefore when designing an algorithm for detection, it is necessary to have both good detection capabilities and a low enough false alarm rate to be used by the client. In foliage penetration applications, the primary source of clutter comes from large tree trunks, and the more sparse the forest is, the less the number of false targets will be. It is also true that the larger the tree, the higher the number of false alarms \cite{Book_ML}.

According to \cite{Carabas} VHF band has good performance for detecting targets under trees, but there are very few VHF SAR systems in the world. To overcome this problem, researchers from the FOI released a VHF-band SAR image dataset to the public to foster the development of change detection methods for wavelength-resolution SAR systems in the VHF band, which is the dataset used in this work.

\subsection{TanDEM-X dataset}
TerraSAR-X (TSX) and TanDEM-X (TDX), launched in June 2007 and June 2010,
respectively, are two German SAR satellites operating in X-band, developed within a
public/private partnership between the German Aerospace Center (DLR) and Airbus
Defense and Space.
The goal of both satellites is to provide SAR products for commercial and scientific purposes, and the TanDEM-X mission has the primary goal of generating a global,
high precision, and consistent digital elevation model (DEM) with full coverage and no
gaps. The relevance of the mission lies in that, until now, the available DEMs of large
parts of Earth are of low resolution, inconsistent, incomplete and commonly based on
different data sources and survey methods.
TanDEM-X has offered, for the first time, a global, high accuracy and homogeneous DEM.
Besides the primary goal, other secondary missions based on along-track interferometry have
been defined, and techniques with bistatic SAR \cite{Alberto}.

Both satellites have a center frequency of 9.65 GHz, and can operate in different modes: StripMap, ScanSAR, Spotlight, staring SpotLight and TopSAR. The satellites also can use different antenna modes to acquire single, dual and full polarimetric data. The main systems parameters can be seen in Table \ref{tab:tandem_params}.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline 
        \multicolumn{2}{|c|}{System Parameters} \\ 
        \hline \hline
        Center frequency & 9.65 Ghz  \\ \hline
        Bandwidth & 300 Mhz  \\ \hline
        Antenna size & 4.8 m in Azimuth, 0.7 m in elevation  \\ \hline
        Polarization & H and V (single, dual and quad)  \\ \hline
        Look angle range & 15-60 degrees  \\ \hline
        Nominal operation modes & Spotlight, Stripmap, ScanSar  \\ \hline
        Ground resolution & 0.25 m (Spotlight) - 40 m (ScanSAR) \\ \hline
        Scene size (range) & 10 km (Spotlight) - 100 km (ScanSAR)  \\ \hline
        Scene size (azimuth) & 5 km (Spotlight) - 150 km (ScanSAR)  \\ \hline
        Pulse repetition frequency & 2kHz - 6.5 kHz \\ \hline
    \end{tabular}
    \caption{TerraSAR-X and TanDEM-X System Parameters}
    \label{tab:tandem_params}
\end{table}

For more information regarding the TSX and TDX systems the reader is referred to \cite{Alberto}.

The TanDEM-X system provides very high-resolution image data, which can create problems in processing the data since the data will be very large memory-wise. Because of that, it was chosen to work with small areas in Rondônia State. Two images were selected over the Rondônia state with a nice mix of forest and deforested areas. The two areas can be seen in Figure \ref{fig:tandem_dataset}, where each colored rectangle (in colors blue and red) represents one image acquisition of the TanDEM-X system. The center of the two images has coordinates (-10.720611498968696 N, -61.269449884800494 W).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{Cap2-Methods/tandem_dataset.jpg}
    \caption{The red and blue dashed lines delimit the ground areas of the SAR TanDEM-X dataset used in this thesis. The image referred to a location in Rondônia, where the center of the two areas has coordinates -10.720611498968696 N, -61.269449884800494 W.}
    \label{fig:tandem_dataset}
\end{figure}{}

\subsection{Sentinel-1 dataset}

Sentinel-1 is the first new space component of the Global monitoring for environment and security (GMES) satellite family, designed and developed by the European Space Agency (ESA) and funded by the European Commission. Sentinel-1 is composed of two twin satellites, Sentinel-1A and Sentinel-1B, sharing the same orbital plane with 180 degrees orbital phase difference. Its mission is to provide continuous all-weather, day-and-night imagery at C-Band(5.4 GHz). The SENTINEL-1 constellation provides high reliability, improved revisit time, geographical coverage, and rapid data dissemination to support operational applications in the priority areas of marine monitoring, land monitoring, and emergency services.

Sentinel-1 acquires images of all global landmasses and coastal zones and covers ocean mapping at six-day intervals. Different modes can be used to map landmasses and ocean mapping. The main operational mode features a swath of 250 km with a resolution suited for most applications. The Sentinel-1 data products distributed by ESA include raw level data (for specific usage), single look complex (distribution limited), ground range detected data with multi-looked intensity (systematically distributed), and ocean data for retrieved geophysical parameters of the ocean (systematically distributed).

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline 
        \multicolumn{2}{|c|}{System Parameters} \\ 
        \hline \hline
        Center frequency & 5.405 GHz   \\ \hline
        Bandwidth & 100 MHz  \\ \hline
        Antenna size & 12.3 m in Azimuth, 0.821 m in elevation  \\ \hline
        Polarization & H and V (single, dual and quad)  \\ \hline
        Look angle range & 20-46 degrees  \\ \hline
        
        Nominal operation modes & 
        \vtop{\hbox{\strut Stripmap, Extra Wide Swath(EWS)}\hbox{\strut Interferometric Wide Swath(IWS)}}
        \\ \hline

        Ground resolution & 5 m (Stripmap) - 40 m (EWS) \\ \hline
        Scene size (range) & 1000 - 5000 km \\ \hline
        Scene size (azimuth) & 80 km (Stripmap) - 400 km (EWS)  \\ \hline
        Pulse repetition frequency & 1 - 3 kHz \\ \hline
    \end{tabular}
    \caption{Sentinel-1 System Parameters \cite{sentinelRef}}
    \label{tab:sentinel_params}
\end{table}

For more information regarding the Sentinel-1 system the reader is referred to \cite{sentinelmission}.

The images acquired by Sentinel-1 are a monthly interferometric time series. The methodology and processing chain for raw image data is based on the one developed in \cite{Rodrigo,Paolo}.
 The preprocessing steps for the generation of the images consist of a coregistration of the images with respect to a common master acquisition, normally selected as the one in the middle of the temporal stack. Afterward, the backscatter value and coherence were estimated by using a convolutional filter with a window size of 5x19 pixels. Since the resolution of the Sentinel-1 in Interferometric Wide swath mode is 14 m X 3.7 m, this creates a final image with a resolution of approximately 70 m X 70 m. The model used for extracting the temporal correlation is described in \cite{Paolo}.

The study area covered is over a region in the Rondônia state in Brazil. The area covers the region between south latitudes 7$\degree$50$'$ and 13$\degree$50$'$, and west longitudes 59$\degree$50$'$ and 67$\degree$10$'$. Since this is an area of huge deforestation focus, the ESA has set up a 6-day repeat pass coverage with the Sentinel-1 system. For this study area it was downloaded and processed a set of 12 S-1 time series according to the framework explained in Figure \ref{fig:sentinelStack}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Cap2-Methods/sentinelstack.jpg}
    \caption{Sentinel-1 stack of acquisition images. There are four different orbits of acquisitions and each orbit provided 5 images, resulting in 20 total images (of which only 12 were selected). A dot represents a master image (the image used as a master for the coregistration method), and the other images are slave images.}
    \label{fig:sentinelStack}
\end{figure}{}

In Table \ref{tab:sentinelStackTable}, it is also possible to see the geographical description of each orbit and image acquired.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c||c|c|c|c|}
        \hline  
        \multicolumn{7}{|c|}{Corner Coordinates} \\ 
        \hline \hline
        \textbf{Stack} & \textbf{Orbit} &\textbf{Name} &\textbf{Lat. Min} &\textbf{Lat. Max} &\textbf{Lon. Min} &\textbf{Lon. Max} \\
        \hline
        1&010&$TS_0$ &9$\degree$40$'$58$"$S &7$\degree$42$'$53$"$S &59$\degree$52$'$18$"$W &61$\degree$44$'$43$"$W \\
        2&010&$TS_1$ &11$\degree$16$'$36$"$S &9$\degree$15$'$31$"$S &60$\degree$12$'$59$"$W &62$\degree$5$'$52$"$W \\
        3&010&$TS_2$ &12$\degree$45$'$21$"$S &10$\degree$43$'$21$"$S &60$\degree$33$'$20$"$W &62$\degree$26$'$54$"$W \\
        4&010&$TS_3$ &9$\degree$40$'$58$"$S &7$\degree$42$'$53$"$S &59$\degree$52$'$18$"$W &61$\degree$44$'$43$"$W \\
        5&054&$TS_0$ &14$\degree$10$'$32$"$S &12$\degree$12$'$43$"$S &60$\degree$53$'$48$"$W &62$\degree$46$'$54$"$W \\
        6*&083&$TS_0$&10$\degree$12$'$15$"$S &8$\degree$5$'$50$"$S &66$\degree$8$'$34$"$W &67$\degree$59$'$40$"$W \\
        7*&083&$TS_1$&10$\degree$22$'$8$"$S &8$\degree$32$'$54$"$S &62$\degree$4$'$44$"$W &63$\degree$37$'$30$"$W \\
        8*&083&$TS_2$&11$\degree$51$'$16$"$S &10$\degree$2$'$26$"$S &62$\degree$25$'$15$"$W &64$\degree$19$'$5$"$W \\
        9*&083&$TS_3$&13$\degree$24$'$3$"$S &11$\degree$32$'$42$"$S &62$\degree$44$'$38$"$W &64$\degree$40$'$34$"$W \\
        10&156&$TS_0$&9$\degree$24$'$34$"$S &8$\degree$4$'$15$"$S &63$\degree$53$'$30$"$W &65$\degree$56$'$2$"$W \\
        11&156&$TS_1$&10$\degree$15$'$7$"$S &8$\degree$48$'$35$"$S &64$\degree$5$'$7$"$W &66$\degree$8$'$22$"$W \\
        21&156&$TS_2$&10$\degree$36$'$21$"$S &9$\degree$46$'$22$"$S &64$\degree$9$'$40$"$W &66$\degree$19$'$6$"$W \\
        \hline
    \end{tabular}
    \caption{Sentinel-1 stack description. The stacks marked with asterisk are images chosen for validation and others are used for the Random Forest Training.}
    \label{tab:sentinelStackTable}
\end{table}

A visual representation of the Rondônia dataset can be seen in Figure \ref{fig:rondoniadataset}. The images acquisitions are overlapped with a Google Maps image of the Rondônia state. From Image \ref{fig:rondoniadataset}, there are three different pixels color that can be seen: green pixels - which represent forest areas - red pixels - which represent deforested areas - and blue pixels - which represent human-made surfaces. This reference map was created by PRODES (Programa de cálculo do desflorestamento da Amazônia) project.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Cap2-Methods/rondonia_dataset.jpg}
    \caption{ Reference deforestation map for the Rondonia dataset provided by PRODES. The blue pixels represent human-made surfaces, the green pixels represent forest areas, and the red pixels represent deforested areas. The yellow number represents the corresponding acquisition orbit.}
    \label{fig:rondoniadataset}
\end{figure}{}

\subsection{CARABAS dataset}

The CARABAS-II is the second generation SAR system designed by the Swedish Defense Research Agency (FOI) for FOPEN applications.
The CARABAS-II has participated in numerous military campaigns and has been used for research purposes since the nineties.
The radar is a VLF UHB SAR system that transmits HH-polarized radio waves in the frequency range of 
20-90 MHz, therefore having a range resolution in the range of 3.3-15 m. The radar antenna is mounted on a Sabreliner aircraft, as seen in \figref{fig:sabreliner}.

\begin{figure}[H]
    \centering
    \includegraphics{chapter6/sabreliner.jpg}
    \caption{The CARABAS-II VHF SAR mounted in front of a Sabreliner airplane.}
    \label{fig:sabreliner}
\end{figure}

In the table \ref{tab:carabas_system} the system parameters for the CARABAS-II system are presented for the
flight campaigns that were used in this work. 

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        System Parameters & Values \\ \hline \hline
        Nominal flight altitude & 3 - 9 km \\ \hline
        Nominal flight speed & 127 m/s \\ \hline
        Frequency band & 20-86 MHz \\ \hline
        Aperture angle & 90 degrees \\ \hline
        Transmitted power & 500 W \\ \hline
        Pulse modulation & Non-linear frequency modulation \\ \hline
        Radio Frequency Interference (RFI) sniff & On \\ \hline
        Frequency sub-bands & 35 (36 with RFI-sniff) \\ \hline
        Frequency step & 1.875 Mhz \\ \hline
        Center frequencies & 21.25-85Mhz \\ \hline
        Pulse repetition & frequency 5000Hz \\ \hline
        Pulse length & 15$\mu$s \\ \hline
        Maximum range & 26.4 km \\ \hline
    \end{tabular}
    \caption{CARABAS-II SAR system parameters. Source: \cite{76}}
    \label{tab:carabas_system}
\end{table}

By trying to promote research on change detection algorithms for wavelength-resolution
images, FOI created a dataset of images acquired by CARABAS-II and made it publicly available. 
This dataset is used to test and assess the quality of the proposed CDA.

The dataset consists of 24 SAR images selected from over 150 images obtained during different flight campaigns.
Each image covers the same ground area of 6 $km^2$ (3 km vertically and 2 km horizontally)
and is given in the form of a 3000 X 2000 matrix, where each pixel size is 1 km x 1 km.
According to \cite{ 77,62,78}, the images are already calibrated, pre-processed, and geocoded.

The location of the image dataset is inside the military base station Missile Test Area North
Vidsel in northern Sweden in 2002. The test site is a region near the village of Nausta \cite{75}.
The vegetation of the area is dominated by Scots Pine tree \cite{ 76}, which consists of small and medium size trees.
According to \cite{75} the area also contains fields, roads and lakes.

With the objective of testing CDA quality, it was deployed 25 testing targets over the testing with different configuration 
and arrangements. The testing targets consist of ten TGB11 model military vehicles, eight TGB30 model 
military vehicles, and seven TGGB40 model military vehicles. The dimensions of each vehicle are presented in the table \ref{tab:vehicle_dimensions}. Since those objects
are large (compared to the bandwidth of the signal), and stationary, then their radar
signature will be very stable between acquisitions (ULANDER, 2004). This will be used
as an advantage by taking acquisitions with different flight passes to suppress clutter noise from tree canopy.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline 
        Military Vehicle & Length & Width & Height & Quantity \\ \hline \hline
        TGB11 & 4.4 m & 1.9 m & 2.2 m & 10 \\ \hline
        TGB30 & 6.8 m & 2.5 m & 3.0 m & 8 \\ \hline
        TGB40 & 7.8 m & 2.5 m & 3.0 m & 7 \\ \hline
    \end{tabular}
    \caption{Target information}
    \label{tab:vehicle_dimensions}
\end{table}


The dataset of 24 images were acquired using four different flight passes. Each 
flight pass has an incidence angle of 58 degrees, used the Strip SAR mode and was acquired with the radar looking left \cite{ 75,76}.
The vehicles were positioned in two different forests, Forest 2 being in the northwest of the field, and forest 1 being in the
southeast of the test area. 

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.9]{chapter6/carabas_vehicles_fisico.jpg}
    \caption{Military vehicles used as targets. (left) TGB11. (middle) TGB30. (right) TGB40. 
    This picture also depicts the vegetation characteristics of the test area.}
    \label{fig:veiculos}
\end{figure}

The vehicles in mission 2 are positioned in forest 2 and have a heading angle of 225 degrees pointing southwest direction;
vehicles in mission 3 are positioned in forest 2 and have a heading angle of 315 degrees pointing northwest direction;
vehicles in mission 4 are located in forest 1 and have the same heading angle of mission 2;
vehicles in mission 5 are located in forest 1 and have a heading angle of 270 degrees pointing west direction.
Vehicles in mission 2 and 3 are separated approximately by 50 m, as such for vehicles in mission 4 and 5.
\figref{fig:carabas_vehicles} presents images of each mission with the vehicles area highlighted in red.

\begin{figure}[H]
    \centering
    \includegraphics{chapter6/carabas_vehicles.jpg}
    \caption{Examples of CARABAS-II images. (a) is mission 2, (b) is mission 3, (c) is mission 4 and (d)
    is mission 5.}
    \label{fig:carabas_vehicles}
\end{figure}

In table \ref{tab:flight_mission} it is presented the summary with the information of each image in the dataset

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline 
        Image Number & Mission & Pass & Flight Heading (degrees) & Target Heading \\ \hline \hline
        1 & 2 & 1 & 225 & 225  \\ \hline
        2 & 2 & 2 & 135 & 225  \\ \hline
        3 & 2 & 3 & 225 & 225  \\ \hline
        4 & 2 & 4 & 135 & 225 \\ \hline
        5 & 2 & 5 & 230 & 225 \\ \hline
        6 & 2 & 6 & 230 & 225 \\ \hline
        7 & 3 & 1 & 225 & 315 \\ \hline
        8 & 3 & 2 & 135 & 315 \\ \hline
        9 & 3 & 3 & 225 & 315 \\ \hline
        10 & 3 & 4 & 135 & 315 \\ \hline
        11 & 3 & 4 & 135 & 315 \\ \hline
        12& 3 & 6 & 230 & 315  \\ \hline
        13 & 4 & 1 & 225 & 225  \\ \hline
        14 & 4 & 2 & 135 & 225  \\ \hline
        15 & 4 & 3 & 225 & 225  \\ \hline
        16 & 4 & 4 & 135 & 225  \\ \hline
        17 & 4 & 5 & 230 & 225  \\ \hline
        18 & 4 & 6 & 230& 225 \\ \hline
        19 & 5 & 1 & 225 & 270  \\ \hline
        20 & 5 & 2 & 135 & 270  \\ \hline
        21 & 5 & 3 & 225 & 270  \\ \hline
        22 & 5 & 4 & 135 & 270  \\ \hline
        23 & 5 & 5 & 230 & 270  \\ \hline
        24 & 5 & 6 & 230 & 270  \\ \hline
    \end{tabular}
    \caption{Measurements parameters for each image}
    \label{tab:flight_mission}
\end{table}


\section{Methods}
\subsection{Texture methods for image analysis}

As previously mentioned, the textures might give useful information for classification because sometimes the neighborhood of a pixel also provides valuable information for the classification. In forest landscapes, for example, the texture value might depend on the size and distance between trees, such that in high-resolution images if two pixels fall in the same tree then they will have similar value, resulting in a small local variance of intensities, something that will be indicated by the texture value. According to \cite{Woodcock} if the resolution is increased to a size comparable to the size to of trees, then the local variance also increases, something noticeable in tropical forests with a high species diversity. Therefore, is important to mention that the texture is dependent of the resolution of the image, and the texture of a high resolution image of an area might be different from a low resolution image of the same area.

On this work it will be used three different textures methods for improving classification results: the Grey level co-occurrence matrix(GLCM) method, the Laws textures method and the sum and different histograms methods.

\subsubsection{The GLCM Method}
\label{sec:GLCM_Method}

The first method of texture creation is the grey level co-occurrence matrix (GLCM) method.
A co-occurrence matrix is a matrix extracted from an image in which the values in the rows and columns of this matrix represent the set of possible grey scale values of the image. 
For example, the co-occurrence matrix $C$ is a matrix in which the elements represent the possible co-occurrence of values for the image $I$ given a spatial relationship on a set of possible values $V$. 
For example, given an image, the co-occurrence matrix $C$ indicates how many times the value $i$ co-occurs with the value $j$ given a spatial relationship. 
This spatial relationship is given by a displacement vector $d = (d_r, d_c)$ that dictates the distance of the pixels that one wants to analyze the co-occurrence of values.
A more mathematical way to express this matrix is given by the following definition:

\begin{equation}
    C_{d}(i,j) = \# \{(r,c) | I(r,c)=i\  \text{and} \  I(r+d_r, c+d_c)=j \} 
\end{equation}

For example, if the grey scale image $I$ is equal to

\begin{equation}
I=
    \begin{bmatrix}
    1&1&0&0\\
    1&1&0&0\\
    0&0&2&2\\
    0&0&2&2
    \end{bmatrix} 
\end{equation}{}


Then three co-occurrence matrices for different displacement vectors $d=(0,1)$, $d=(1,0)$ and $d=(1,1)$ are:
\begin{equation}
    C_{(0,1)}=
    \begin{bmatrix}
    4&0&2\\
    2&2&0\\
    0&0&2
    \end{bmatrix}
\end{equation}{}

\begin{equation}
C_{(1,0)}=
    \begin{bmatrix}
    4&0&2\\
    2&2&0\\
    0&0&2
    \end{bmatrix}
\end{equation}{}

\begin{equation}
C_{(1,1)}=
    \begin{bmatrix}
    2&0&2\\
    2&1&1\\
    0&0&1
    \end{bmatrix}
\end{equation}{}

In $C_{(0,1)}$ the position $(0,0)$ equals 4, which means that $j=0$ appears to the right of $i=0$ four times in the image. The position $(1,0)$ equals 2 because $j=0$ appears to the right of $i=1$ two times in the image. The position $(2,1)$ equals 0 because $j=1$ never appears to the right of $i=2$.

From this grey level co-occurrence matrix it is important to generate the \textit{normalized} grey level co-occurrence matrix $N_d$ given by

\begin{equation}
\label{eq:normalized}
    N_d(i,j) = \frac{C_d(i,j)}{\sum_i\sum_j C_d(i,j)}
\end{equation}
Co-occurrence matrices capture the texture properties, but are not directly useful for further analysis, such as comparing different textures. To solve this problem, is possible to compute numeric features from this normalized co-occurrence matrix that might give useful information of the image that otherwise would be hidden for machine learning classification algorithms. These numeric features computed from the normalized co-occurrence matrix represent more compactly the textures of the image.
The following textures are features that can be obtained from normalized co-occurrence matrix:
\begin{table}[H]
\centering
\begin{tabular}{ |c |c |}
 \hline Contrast & $\sum_{i,j=0}^{N-1} i\frac{N_d(i,j)}{1+(i-j)^2}$\\ 
 \hline Dissimilarity & $\sum_{i,j=0}^{N-1} i N_d(i,j)|i-j|$\\
 \hline Homogeneity & $\sum_{i,j=0}^{N-1}\frac{N_d(i,j)}{1+|i-j|}$\\
 \hline Angular\ Second\ Moment & $\sum_{i,j=0}^{N-1} i N_d(i,j)^2$\\
 \hline Correlation & $\sum_{i,j=0}^{N-1}\frac{(i-\mu)(j-\mu)N_d(i,j)}{\sigma^2}$\\
 \hline Energy & $\sum_{i,j=0}^{N-1}N_d(i,j)^2$\\
 \hline Entropy & $ -\sum_{i,j=0}^{N-1}N_d(i,j)\log_2(N_d(i,j))$\\
 \hline
\end{tabular}
\caption{Formula for texture calculations for GLCM Method}
\label{table:glcm_calculations}
\end{table}

where $\mu$ is the GLCM mean and $\sigma$ is the standard deviations of the GLCM given by:

\begin{equation}
    \mu=\sum_{i,j=0}^{N-1}iN_d(i,j)
\end{equation}
\begin{equation}
    \sigma^2=\sum_{i,j=0}^{N-1}N_d(i,j)(i-\mu)^2
\end{equation}

A question that rises from this method is how to choose the displacement vector $d=(d_r,d_c)$ such that the result is the best. A solution proposed by  \cite{Zucker} is to run a hypothesis test to check the displacement vector $d$ that most rejects the hypothesis that the pair of pixels separated by the displacement vector $d$ are independent, therefore accepting the hypothesis that this pair of pixels give the most information one of the another and are better suited to compute a texture feature.
The hypothesis test suggests running a $\chi^2$ statistical test to find the value $d$ that give the most information, that is, to maximize the value:
\begin{equation}
    \chi^2(d) = \left(\sum_i\sum_j \frac{N_d^2(i,j)}{\sum_jN_d(i,j)\sum_iN_i(i,j)} -1\right)
\end{equation}

% \subsubsection{The Laws Textures}
% \label{sec:Laws_Textures}

% A different way to create textures is to use 2D filters to detect different kinds of textures. Laws developed a method to measure the amount of variation of the image on a fixed sized window. The method consists of making 16 different convolutions of the original image with convolution masks and use the result to compute the texture. The 16 different filters are obtained by taking the product of the following vectors.
% \begin{align}
% L5 (Level) = 
% \begin{bmatrix}
% 1&4&6&4&1
% \end{bmatrix}\\
% E5 (Edge) = 
% \begin{bmatrix}
% -1&-2&0&2&1
% \end{bmatrix}\\
% R5 (Ripple) = 
% \begin{bmatrix}
% -1&0&2&0&-1
% \end{bmatrix}\\
% S5 (Spot) = 
% \begin{bmatrix}
% 1&-4&6&-4&1
% \end{bmatrix}
% \end{align}

% Then the 2D convolution masks are obtained by taking the product of these vectors. For example the R5S5 mask is computed as the product of R5 and S5 as following:
% \begin{equation}
% \begin{bmatrix}
% -1\\0\\2\\0\\-1
% \end{bmatrix}
% \times
% \begin{bmatrix}
% 1&-4&6&-4&1
% \end{bmatrix}
% =
% \begin{bmatrix}
% -1&0&2&0&-1\\
% 4&0&-8&0&4\\
% -6&0&12&0&-6\\
% 4&0&-8&0&4\\
% -1&0&2&0&-1
% \end{bmatrix}
% \end{equation}

% After applying the 16 5x5 masks to the image it is necessary to generate the texture energy map on a fixed size window. Let the window size be 15x15 and $F_k[i,j]$ the result of the $kth$ mask on the pixel [i, j]. Then the texture energy map $E_k$ is defined by:
% \begin{equation}
% E_k[r,c] = \sum_{j=c-7}^{c+7}\sum_{i=r-7}^{r+7} |F_k[i,j]|   
% \end{equation}

% After creating the 16 energy maps it is needed to group together similar images that perceive the textures in different directions. For example, if R5S5 detects at ripples in the vertical direction, then S5R5 detects ripples in the horizontal direction, so it is desirable to group them together to detect ripple in both directions. The average of these two images measure then the total ripple content. It is possible to group the 16 energy maps in 9 texture images as following:
% \begin{align*}
% L5E5/E5L5\qquad L5S5/S5L5\\
% L5R5/R5L5\qquad\qquad\ \ E5E5\\
% E5S5/S5E5\qquad E5R5/R5E5\\
% S5S5\qquad\qquad\ \ S5R5/R5S5\\
% R5R5
% \end{align*}


\subsubsection{The Sum and Difference Texture Histograms}
\label{sec:sum_and_diff_hist_texture}
\paragraph{The Single Value Decomposition}
Let $I[k,l]$ be a discrete image which is the realization of a bi-dimensional stationary process and let $G=\{1, 2,...,N_g\}$ be the set of the $N_g$ quantized grey levels.
Just as in the GLCM method, consider a pair of pixels $y_1$ and $y_2$ separated by a displacement vector $d = (d_1, d_2)$ such that
\begin{equation}
\begin{split}
    y_1 &= I[k,l], \text{and}\\
    y_2 &= I[k+d_1, l+d_2]
\end{split}
\end{equation}

The discrete joint probability function of these two pixels is $P(y_1, y_2)$. And the probability of observing a pair of grey level occurrence $i$ and $j$ separated by a displacement vector $(d_1, d_2)$ is given by
\begin{equation}
    Prob\{y_1=i,\  y_2=j\} = P(i,j,d_1,d_2) = P(i,j),
\end{equation}
which does not depend on the pixel position $[k,l]$.

Pay attention to the fact that an estimate of this joint distribution $P[i,j]$ is the normalized co-occurrence matrix given by \ref{eq:normalized}, such that
\begin{equation}
    \hat{P}(i,j) = N_d[i,j] \simeq P(i,j),
\end{equation}
\newline

Let us consider the random vector of the two random variables $y_1$ and $y_2$. Let $Y$ be this random vector such that $Y=[y_1, y_2]^T$. Suppose that this random vector $Y$ has a co-variance matrix $C_{yy}$. The goal is to make a linear transformation on this vector such that the new vector has uncorrelated variables. Suppose the linear transformation is given by a matrix $H$ and the new random vector is $Z$ such that
\begin{equation}
    Z=HY
\end{equation}{}
It is known from \cite{papoulis} that the co-variance matrix $C_{zz}$ of the random variable $Z$ is obtained by the following formula.
\begin{equation}
    C_{zz} = HC_{yy}H^T
\end{equation}
If we make a single value decomposition on the matrix $C_{yy}$ such that $C_{yy} = U\Lambda^2U^T 
=U\Lambda \Lambda U^T$ and we choose the matrix $H$ such that $H = \Lambda^{-1}U^T$ then the co-variance of the random vector $Z$ is given by
\begin{equation}
    C_{zz} = HC_{yy}H^T = \Lambda^{-1}U^T U\Lambda \Lambda U^T U \Lambda^{-1} = I
\end{equation}{}
Therefore the components of Z are uncorrelated.

Let us model the random vector $Y$ to have similar cross-correlation between the variables, such that:
\begin{equation}
    C_{yy} = \sigma_y^2 
    \begin{bmatrix}
    1&\rho\\
    \rho&1
    \end{bmatrix}
\end{equation}{}
where
\begin{equation}
    \sigma_y^2\ \rho = E\{(y_1-\mu)(y_2-\mu)\} \ \text{and} \ \mu = E\{y_1\} = E\{y_2\},
\end{equation}{}
and due to stationarity 
\begin{equation}
    E\{(y_1-\mu)^2\}=E\{(y_2-\mu)^2\} = \sigma_y^2
\end{equation}{}

Let us make then the transformation given by
\begin{equation}
H =\frac{1}{\sqrt{2}} 
\begin{bmatrix}
1&1\\
1&-1
\end{bmatrix}{}
\end{equation}
That results in the new random vector $Z=[z_1, z_2]^T$ that:
\begin{equation}
\begin{split}
z_1 &=\frac{y_1+y_2}{\sqrt{2}}, \text{and}\\
z_2 &= \frac{y_1-y_2}{\sqrt{2}}
\end{split}
\end{equation}
Such that the new co-variance matrix for $Z$ is given by:
\begin{equation}
C_{zz}=
    \begin{bmatrix}
    \sigma_y^2 (1+\rho) & 0\\
    0 & \sigma_y^2 (1-\rho)
    \end{bmatrix}{}
\end{equation}{}

\paragraph{Approximation of Probability Density Functions}
Let $\alpha \in N_g$ and $\beta \in N_g$.
If we assume that the variables for $y_1$ and $y_2$ are gaussian, then $z_1$ and $z_2$ are also gaussian and uncorrelated, therefore the joint probability function can be calculated from

\begin{equation}
P(y_1=\alpha, y_2=\beta) = P(z_1=\alpha + \beta, z_2=\alpha - \beta) = P_s(z_1=\alpha+\beta)\cdot P_d(z_2=\alpha-\beta) 
\end{equation}

If the variables are not gaussian then the equality is not valid, but it is still a valid approximation to the probability function $P(y_1, y_2)$,
\begin{equation}
    \hat{P}_y(i,j) = c_0\cdot P_s(i+j) \cdot P_d(i-j) \simeq P_y(i,j)\text{,}
    \label{approximation_gaussian}
\end{equation}{}
where $c_0$ is a normalization constant that assures that
\begin{equation}
    \sum_{i=1}^{N_g}\sum_{j=1}^{N_g} \hat{P}_y(i,j) = 1
\end{equation}

The relative error of this approximation can be computed analyzing the Kullback-Leibler Divergence between the two variables $P$ and $\hat{P}$ as follows
\begin{equation}
\label{eq:divergence}
    I(P, \hat{P}) = \sum_i\sum_j P_y(i,j)\cdot log(\frac{P_y(i,j)}{\hat{P}(i,j)}) = \\
    H_s + H_d - H_y -log(c_0)\geq0
\end{equation}

where $H_s$, $H_d$ and $H_y$ are the entropies given by
\begin{equation}
\begin{split}
H_y &= -\sum_{i=1}^{N_g}\sum_{j=1}^{N_g}P_y(i,j)\cdot log(P_y(i,j)\\
H_s &= -\sum_{k=2}^{2N_g}P_s(k)\cdot log(P_s(k)\\
H_d &= -\sum_{l=-N_g+1}^{N_g-1}P_d(l)\cdot log(P_d(l))
\end{split}
\end{equation}
This divergence is a measure of the error of approximating one distribution for another, and is equal to zero only when the two variables have the same distribution, therefore it will only be zero if $P(i,j)=\hat{P}(i,j)$. The importance of (\ref{eq:divergence}) is that it is possible to see the independence of the new variables by comparing only the entropy of the sum and difference to the entropy of the co-occurrence matrix. The closer the mutual information $I(P, \hat{P})$ is to zero, the better the approximation defined by (\ref{).approximation_gaussian}

\paragraph{Textures Features}
It was shown that the sum and difference are a linear transformation that uncorrelates the random variables of the pair of the pixels. Therefore, it is suggested by \cite{Unser} to replace the regular co-occurrence matrix with the associated histograms of the sum and difference estimated from the original image.
The non-normalized sum and difference associated with the displacement vector $d=(d_1,d_2)$ are given by:
\begin{equation}
\begin{split}
&s_{k,l} = I[k,l] + I[k+d_1, l+d_2]\\
&d_{k,l} = I[k,l] - I[k+d_1, l+d_2]
\end{split}
\end{equation}

And then we make the normalized histograms of these new vectors. The histograms of the sum and difference are given by:
\begin{equation}
\begin{split}
&h_s(i, d_1, d_2) = h_s(i) = |\{(k,l) \in G, \ s_{k,l} = i\}|\\
&h_d(j, d_1, d_2) = h_d(j) = |\{(k,l) \in G, \ d_{k,l} = j\}|
\end{split}
\end{equation}
And the normalized sum and difference histograms are given by:
\begin{equation}
\begin{split}
&\hat{P}_s(i) = \frac{h_s(i)}{\sum_i h_s(i)} \qquad (i=2,...,2N_g)\\
&\hat{P}_d(i) = \frac{h_d(i)}{\sum_j h_d(i)} \qquad (j=-N_g+1, ... ,N_g-1)
\end{split}
\end{equation}
And these normalized sum and difference histograms are estimates of the sum and difference probability functions given by:
\begin{equation}
\begin{cases}
&P_s(i) = Prob\  \{s_{k,l} = i\} \qquad (i=2,...,2N_g)\\
&P_d(j) = Prob\  \{d_{k,l} = j\} \qquad (j=-N_g+1, ... ,N_g-1)
\end{cases}
\end{equation}

From these estimates of the probability density functions, it is possible to compute textures. Nine important textures will be used in this work. In Table \ref{table:sum_and_diff_calculations}, there are the computation formulas for them

\begin{table}[H]
\centering
\begin{tabular}{ |c |c |}
 \hline Mean & $\frac{1}{2}\sum_i i\hat{P}_s(i)$\\ 
 \hline Variance & $\frac{1}{2}(\sum_i (i-2\mu)^2\hat{P}_s(i) + \sum_j (j)^2\hat{P}_d(j))$\\
 \hline Energy & $\sum_i \hat{P}_s(i)^2 \ \sum_j \hat{P}_d(j)^2$\\
 \hline Correlation & $\frac{1}{2}(\sum_i (i-2\mu)^2\hat{P}_s(i) - \sum_j (j)^2\hat{P}_d(j))$\\
 \hline Entropy & $-\sum_i\hat{P}_s(i)log_2(\hat{P}_s(i)) - \sum_j\hat{P}_d(j)log_2(\hat{P}_d(j))$\\
 \hline Contrast & $\sum_j j^2 \hat{P}_d(j)^2$\\
 \hline Homogeneity & $\sum_j\frac{1}{1+j^2} \hat{P}_d(j)$\\
 \hline Cluster shade & $\sum_i (i-2\mu)^3 \hat{P}_s(i)$\\
 \hline Cluster prominence & $\sum_i (i-2\mu)^4 \hat{P}_s(i)$\\
 \hline
\end{tabular}
\caption{Some Sum and Difference Histograms Method}
\label{table:sum_and_diff_calculations}
\end{table}


It is also important to know which size the window should be to compute the histograms for the sum and difference. Through this work the window size chosen was to be a regular window of size 10x10.
The number of gray levels is also important. Through this work, unless specified, the images will be normalized and scaled manually to have 10 different grey levels.
\\
Theoretically, if the variables were gaussians, then this method would give the same result as the Gray-Level Co-Occurrence matrix method, since if the variables are gaussians are uncorrelated then they are independent and the formula given by $(4.29)$ holds exactly and is not just and approximation. If they are not gaussians then the approximation is not perfect, but there is a great computational advantage to use this method, since making histograms of 1-D vectors is much faster than extracting a 2-D co-occurrence matrix.

\subsection{Machine learning methods to image analysis}

Machine Learning methods are currently among the best tools for problem-solving. While there are many types of learning and subdomains, most share some characteristics, like minimizing a loss function, using data to extract information, and undergoing a training process to "fit" the data. These algorithms are also subject to the same underlying problems faced and dealt with in this work, such as over/underfitting, the bias-variance trade-off, hyperparameters tunning, and regularization.

Machine Learning revolutionized the field of Computer Vision even before experts had to handcraft kernels to be used in feature extraction, which often led to a case-by-case scenario, therefore extremely cumbersome and overengineered. Also, the field had severe problems with performance, and the most challenging tasks seemed unreachable using classical methods. Specifically, Deep Learning offered better results to these problems with the cost of being less interpretable and more computationally expensive.

This section shall present some of the most modern algorithms used to tackle challenging perceptual problems. Due to their high dimensionality, images require Machine Learning models that are highly flexible. In this thesis, we propose using Random Forests and Convolutional Neural Networks (CNNs), more specifically a CNN used for segmentation, called U-Net.

\subsubsection{Random Forest}
Random Forests are Machine Learning models built upon simpler models called Decision Trees, which we will briefly discuss to understand the former better.

Decision Trees are basically a graphical representation of if-else rules. In these trees, we start at the root and choose our path downwards by comparing the features with the rules at each node. We keep doing this process until we reach a leaf. They are considered a Machine Learning algorithm because the partition rules are learned, and the tree is usually built in a top-down approach, where we decide on a feature to use as a dividing rule at each node. The chosen feature is selected based on a metric; those metrics usually consist of one of the following: maximizing the accuracy, minimizing the Gini impurity, or optimizing the information gain. In Figure \ref{decision}, we can see an example of a Decision Tree.

\begin{figure}[H]
    \includegraphics[width=.7\textwidth]{Cap2-Methods/Decision_Tree.jpg}
    \centering
	\caption{Decision Tree applied to the classical Titanic dataset. Image taken from \cite{decisionimg}.}
	\label{decision}
\end{figure}

Decision Trees are notoriously easy to interpret and inexpensive to build, but unfortunately, they are inaccurate \cite{elements}. We use some well-known techniques from the Machine Learning community to improve this. We use a method called bagging, in which we repeatedly select some subset of the available data (with replacement) to fit a Decision Tree. Still, at each node of the tree, we select a random subset of the available features to be used as a partitioning rule. After this process, we will have many Decision Trees, and the prediction for a new input will be decided by taking the vote of the trees. This built model is called a Random Forest. The illustration of a Random Forest can be seen in Figure \ref{random}.

\begin{figure}[H]
    \includegraphics[width=0.7\textwidth]{Cap2-Methods/Random_forest_diagram_complete.png}
    \centering
	\caption{A Random Forest being used for classification. Image taken from \cite{randomimg}.}
	\label{random}
\end{figure}

\subsubsection{Convolutional Neural Networks}

Convolutional Neural Networks are specialized Artificial Neural Networks (ANN). In the usual form of the ANN, we have each neuron connected to all the neurons in the previous and subsequent layers, as illustrated in Figure \ref{ann}. This characteristic of an ANN makes the number of connections grow combinatorically large with input/latent dimensions, making it unfeasible to work with images.


\begin{figure}[H]
    \includegraphics[width=8cm]{Cap2-Methods/rede.png}
    \centering
	\caption{Representation of an Artificial Neural Network. Image taken from \cite{neuralimg}.}
	\label{ann}
\end{figure}

In order to solve this problem, in 1988 it was firstly proposed a Convolution Neural Network \cite{cnn_gordo}, where, instead of learning all the connections, we would learn weights of a convolutional kernel, which will be applied locally to the image. Despite the great idea, CNNs did not see much success until the advent of AlexNet \cite{alex}, where CNNs were successfully used for image classification and, since then, have become the go-to norm in perceptual computer vision tasks. 

CNNs work in a sliding window fashion, where each neighborhood contributes for a single pixel in the next layer. This encodes our prior belief that neighbor pixels should influence each other, which is not always true, but greatly reduces the amount of memory needed to process our input making it functional to work with images. The representation of the mechanism of a CNN can be seen in Figure \ref{cnn}.

\begin{figure}[H]
    \includegraphics[width=\textwidth]{Cap2-Methods/image2.png}
    \centering
	\caption{Representation of a Convolutional Neural Network. Image taken from \cite{cnnimg}.}
	\label{cnn}
\end{figure}

Another key technique used with CNNs is the pooling operation, where we take regular spaced grids in the image and condense the spatial information contained in them. The most common form of pooling is the max-pooling, which takes only the maximal value at each grid. This procedure is illustrated in Figure \ref{pooling}.

\begin{figure}[H]
    \includegraphics[width=.7\textwidth]{Cap2-Methods/MaxpoolSample2.png}
    \centering
	\caption{Example of max pooling. Image taken from \cite{poolimg}.}
	\label{pooling}
\end{figure}

This makes the learned representation more translation-invariant \cite{dlbook}, which is desirable in some applications. Pooling is used to aggregate \textbf{semantic} information at the cost of geometric information since we lose track of the position of the maximal pixel. Another effect of pooling is quickly shrinking the image, which reduces the number of parameters needed for the network.

\subsubsection{U-Net}

The UNET is a convolutional neural network that was developed in the Computer Science department of the University of Freiburg for the purpose of performing image segmentation for
biomedical pictures \cite{Unet}. It became popular because it was able to perform precise segmentations with little time (segmentation of a 512x512 image takes less than a second using a modern GPU).
Convolutional Neural networks normally work by performing a series of convolutions and pooling filters that decrease the size of the image, while the UNET works by also 
supplementing this usual operation pipeline with a series of successive layers in which the pooling operations are replaced by up sampling operators, therefore increasing the resolution of the output (a step called \textbf{transposed convolution}).
The authors also used \textbf{skip connections} where we connect one part of the network directly into another (using sums or concatenation). An overview of the network can be seen in Figure \ref{unetimg}.
The final goal is that a usual successive convolutional layer learn to create an output based on this information. 

It is important to remind that an important aspect of the UNET is that there is many feature channels responsible for the up sampling step, which ensures that the UNET propagates information
to the higher resolution layers. After the expansive path is performed, it is combined with the usual pooling operations, creating a contractive path, which makes the UNET shows an u-shaped architecture. 
It is important also to mention that in the borders of the image, the UNET will extrapolate by mirroring the information to predict the pixels in the area. 
\ref{unetimg} depicts the architecture of the UNET, and it's u-shape.

\begin{figure}[H]
    \includegraphics[width=.9\textwidth]{Cap2-Methods/screenshot_1.png}
    \centering
	\caption{U-Net original architecture. Image taken from \cite{unet_gordo}.}
	\label{unetimg}
\end{figure}

The key takings from this work were the use of transposed convolutions exploiting the strong semantic information acquired by the network linked to the geometrical information that is stored in the downward stream of the network, the combination of these pathways granted U-Net the first place in the EM segmentation challenge. Another important point to the success of this approach is the use of skip connections to improve the backward gradient update. Since the gradient has to move a shorter path to reach the earlier layers, it usually avoids the common vanishing and exploding problems.

The proposed CDA is based on this UNET architecture and has two phases: the training phase and the inference phase.

\textit{A. Training Phase}
\newline
In this phase the model is treated like a regular supervised machine learning model. The given UNET architecture is trained in a supervised manner with the reference map
to be able to perform semantic image segmentation. 
\newline
\textit{B. Inference Phase}
\newline
This phase is based on the Change Detection algorithm proposed by \cite{Kevin}. This algorithm was based on modifying the original UNET two accept two images for the purpose of change detection.
The original algorithm has 5 feature maps created by the convolutions that were generated and saved for the first image. After that, when the second image is used as input, a Difference Image (DI) is
created at each of the 5 levels using the feature maps of the first and second image.
The DI is created by taking the absolute value of the difference between the feature maps, and setting the difference to be zero if the difference value does not surpass a threshold value (which was found empirically), and setting it to 
the value of the second image if it surpasses the threshold value. Intuitively this step works by setting it to zero all pixels that did not change sufficient (according to a threshold value). After that, the DI is semantically segmented, rendering a visual 
interpretation of the collective changes that happened. A visual representation of this algorithm is show in \figref{fig:kevin_algorithm}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=.9\textwidth]{Chapter7/kevin_algorithm.jpg}
    \caption{Inference Phase for the corresponding DI.}
    \label{fig:kevin_algorithm}
\end{figure}

The CDA of this work is inspired by the algorithm explained above. Since the textures provide additional information, it would be very hard to empirically achieve the values for the threshold of each layer (in our case, it would be a vector instead of a scalar), so it was decided to perform the difference image just for first input layer of the UNET. We used the textures of Entropy and Variance, which were combined with the original image to create a three-channel tensor that was fed and trained on the UNET.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{Chapter7/diagrama.png}
    \caption{ Diagram of the proposed algorithm.}
    \label{fig:diagrama}
\end{figure}

After creating the 24 image tensors, the dataset was divided into 12 tensors for training and 12 tensors for validation purposes. After the training was performed, this trained UNET was used as part of the CDA. The proposed CDA has 4 steps and works as follows
\begin{enumerate}
    \item Feed the first tensor as an input to the UNET and get the classification
    \item Feed the second tensor as input to the UNET, and after that alternate the classes from the final image (switch all car classification pixels by forest classification pixels and vice-versa)
    \item Take the absolute value of the difference of the tensors, perform a bias correction (the value for the bias correction was adjusted manually). After that feed this tensor as input to the UNET.
    \item Get the three classifications obtained prior and take the intersection of all classes (in order to a pixel to be classified as a car in the final image, it has to be a car in all the three images at the same time)
\end{enumerate}

Item 2 was added to the algorithm simply to made it more robust against false errors. By adding another necessary step to identify the change as a target change difference
we decrease the probability of false detection, but at the same time it will also decrease the overall probability of true detection for the algorithm. 

After the final image is created by the CDA, a threshold filter selection was performed to eliminate noise classification from the image - we considered only sets of pixels which had 100 or more connected components.

In \figref{fig:diagrama} the proposed algorithm is represented visually.

