This chapter will give a brief introduction of the knowledge that is needed in order to fully understand the problem that we are trying to tackle, how we are going to solve it, and how to better interpret the results. The chapter will start by giving a description of the satellites systems that were used throughout this thesis, the dataset that was provided by each satellite, and each problem that will be tackled with each dataset - for the Amazon Rainforest dataset the problem that will be tackled is the problem of deforestation mapping and detection, meanwhile for the Swedish forest dataset, the problem that will be tackled is the change target detection problem. 

The chapter will also give an introduction about the methods and techniques that one must know in order to understand the approach that was used to solve the problems; more specifically, machine learning techniques such as: random forests, neural networks, and convolutional neural networks, and signal processing techniques such as statistical signal processing and feature texture extraction.

\section{Materials}
This section will give details about the satellite systems used - the German Tandem-X satellite from DLR, the European Sentinel-1 satellite from ESA, and the Swedish CARABAS system from FFA. It will also give details about the test areas chosen to perform the study.

For the deforestation detection problem it was chosen to use data from the Amazon Rainforest, focusing specifically in the Rondônia state area. Rondônia is a state in northern Brazil that is topographically composed mainly by flat lands, and plateaus with low altitude. The climate in Rondônia is called "humid equatorial", which means that the temperature variation throughout the year is very little (something that is very desirable when working with SAR acquisitions, since it is preferable to have as much little variation as possible between acquisitions) \cite{rondoniaGeography}. The pluviometric indexes in the state can reach up to 2100 mm per year, with most of the rains happening between May and September - something that one must know to better select and compare data since the rainy season can affect the image acquisition, so one must always use images from the same season when comparing acquisitions. 

One of the goals of this thesis is to improve algorithms for deforestation mapping and detection, therefore Rondônia was a natural choice for a study area since it is the state with the most deforestation from the Amazon Rainforest. The Rondônia
state already lost over 31\% of its forests and most of the remaining areas are degraded.
For comparison, Acre, the state which borders Rondônia on the west, has 91\% of its
original forest cover and a greater part of it is still intact \cite{rondoniaDeforestation}. 

The deforestation in the Rondônia state can be easily seen with optical satellite data
acquired with Google Earth. Deforestation follows a fairly predictable pattern, as seen in
Figure 4.1. The pattern of deforestation is known as fishbone pattern due to its similarity
with a fishbone skeleton. This pattern arises from the fact that deforesting is normally done
by penetrating the forest and then deforesting along the edges of the road firstly created. Due to recent fires that happened in the year of 2019 in the Amazon Rainforest, there
is much concern about studying and monitoring the deforestation that happens in that
area, and considering that the Rondônia state is the area in which the deforestation is
most critical, it was a natural choice of area to study for this thesis.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Cap2-Methods/fishbone.png}
    \caption{Fishbone pattern of deforestation}
    \label{fig:fishbone}
\end{figure}


Another approach that we can take to solve the problem of monitoring deforestation is anticipating when deforestation will take place instead of monitoring it after it has happened. One approach that one can take is trying to detect vehicles used for deforestation that might be concealed under tree foliage. 

There are many situations were there is a need for monitoring vehicles concealed by foliage over a large area.
For example, for military applications, it might be necessary to monitor the positioning of enemy vehicles trespassing
a restricted area. For environmental purposes it might be needed to pinpoint the position of vehicles that are being used
for illegal deforestation overprotected forests. If SARs are to be used for such purposes, it is ideal that low frequency systems
are chosen since those provide a wide surveillance area and good foliage penetration capabilities. 
At low frequency (VHF-band) the main backscatter from the target area is due to large scatters, e.g. tree canopy, houses,
vehicles, and other man-made objects, which will appear as very bright objects in the image.

The challenge related to change detection is associated with the trade-off between having high accuracy detection and high false alarm rate.
Normally what happens is by trying to increase the overall accuracy of detection algorithms will incur in having a high number of false alarms, 
therefore when designing an algorithm for detection it is necessary to have both good detection capabilities and a low enough false alarm rate
to be used by the client. In foliage penetration applications, the main source of clutter comes from large tree trunks, and the more sparse the
forest is the less will be the number of false targets. It is also true that the larger the tree then the higher will be the number of false alarms \cite{Book_ML}.

The objects that will be used to assess the quality of the change detection method will be a set of 25 military vehicles distributed among a testing area.
Since those objects are large (compared to the bandwidth of the signal), and stationary, then their radar signature will be very stable between acquisitions \cite{63}.
This will be used as an advantage by taking acquisitions with different flight passes to suppress clutter noise from tree canopy.

According to \cite{Carabas} VHF-band has good performance for detecting targets under trees, but there are very few VHF SAR systems in the world.
To overcome this problem researchers from the FOI released a VHF-band SAR image dataset to the public to foster the development of 
change detection methods for wavelengh-resolution SAR systems in VHF band, which is the dataset that will be used in this work.

For the target change detection problem the dataset used are 24 image acquisitions over a forest in northern Sweden. Unlike for the deforestation detection problem, we had no flexibility over which area to chose to perform the study, since the CARABAS dataset is a free dataset made available by the Swedish Air force Research Center. The CARABAS dataset will be given a further description in a future section of this work. 


\subsection{The Tandem-X system and dataset}
TerraSAR-X (TSX) and TanDEM-X (TDX), launched in June 2007 and June 2010,
respectively, are two German SAR satellites operating in X-band, developed within a
public/private partnership between the German Aerospace Center (DLR) and Airbus
Defense and Space.
The goal of both satellites is to provide SAR products for commercial purposes and
scientific purposes, and the TanDEM-X mission has the primary goal of generating a global,
high precision, and consistent digital elevation model (DEM) with full coverage and no
gaps. The relevance of the mission lies in that, until now, the available DEMs of large
parts of Earth are of low resolution, inconsistent, incomplete and commonly based on
different data sources and survey methods.
TanDEM-X has offered, for the first time, a global, high accuracy and homogeneous DEM.
Besides the main goal, other secondary missions based on along-track interferometry have
been defined as well as new techniques with bistatic SAR \cite{Alberto}. The work presented on this thesis is one of the
possible exploitation of the activities of the TanDEM-X data.

Both satellites have a center frequency of 9.65 GHz, and can operate in different modes: stripMap, scanSAR, Spotlight, staring SpotLight and TopSAR. The satellites also can use different antenna modes in order to acquire single, dual and full polarimetric data. The main systems parameters can be seen in table \ref{tab:tandem_params}.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline 
        \multicolumn{2}{|c|}{System Parameters} \\ 
        \hline \hline
        Center frequency & 9.65 Ghz  \\ \hline
        Bandwidth & 300 Mhz  \\ \hline
        Antenna Size & 4.8 m in Azimuth, 0.7 m in elevation  \\ \hline
        Polarization & H and V (single, dual and quad)  \\ \hline
        Look angle range & 15-60 degrees  \\ \hline
        Nominal operation modes & Spotlight, Stripmap, ScanSar  \\ \hline
        Ground resolution & 0.25 m (Spotlight) - 40 m (ScanSAR) \\ \hline
        Scene size (range) & 10 km (Spotlight) - 100 km (ScanSAR)  \\ \hline
        Scene size (azimuth) & 5 km (Spotlight) - 150 km (ScanSAR)  \\ \hline
        Pulse repetition frequency & 2kHz - 6.5 kHz \\ \hline
    \end{tabular}
    \caption{TerraSAR-X and TanDEM-X System Parameters}
    \label{tab:tandem_params}
\end{table}

For more information regarding the TSX and TDX system the reader is referred to \cite{Alberto}.

The TanDEM-X system has the advantage of being able to provide very high resolution image data, which can create problems to process the data since the data will be very large memory wise. Because of that it was chosen to work with small areas in Rondônia State. There were two images that were selected over the Rondônia state that have a nice mix of forest areas and deforested areas. The two areas can be seen in figure \ref{fig:tandem_dataset} where each colored rectangle (in colors blue and red) represent one image acquisition of the TanDEM-X system. The center of the two images have coordinated (-10.720611498968696 N, -61.269449884800494 W).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{Cap2-Methods/tandem_dataset.jpg}
    \caption{TanDEM-X dataset available in Rondônia Area (Image provided by Google Earth).}
    \label{fig:tandem_dataset}
\end{figure}{}

\subsection{The Sentinel-1 system and dataset}

Sentinel-1 is first new space component of the Global monitoring for environment and security (GMES) satellite family, designed and developed by the European Space Agency (ESA) and funded by the European Commission. Sentinel-1 is composed of two twins satellites, Sentinel-1A and Sentinel-1B, sharing the same orbital plane with 180 degrees orbital phase difference. Its mission is to provide continuous all-weather, day-and-night imagery at C-Band(5.4 GHz). The SENTINEL-1 constellation provides high reliability, improved revisit time, geographical coverage and rapid data dissemination to support operational applications in the priority areas of marine monitoring, land monitoring and emergency services.

Sentinel-1 acquires images of all global landmasses, costal zones, and covers ocean mapping at 6 days intervals. There are different modes that can be used to map landmasses and ocean mapping. The main operational mode features a swath of 250 km with resolution that is suited for most applications. The Sentinel-1 data products distributed by ESA include: raw level data (for specific usage), single look complex (distribution limited), ground range detected data with multi-looked intensity (systematically distributed) and ocean data  for retrieved geophysical parameters of the ocean (systematically distributed). 

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline 
        \multicolumn{2}{|c|}{System Parameters} \\ 
        \hline \hline
        Center frequency & 5.405 GHz   \\ \hline
        Bandwidth & 100 MHz  \\ \hline
        Antenna Size & 12.3 m in Azimuth, 0.821 m in elevation  \\ \hline
        Polarization & H and V (single, dual and quad)  \\ \hline
        Look angle range & 20-46 degrees  \\ \hline
        
        Nominal operation modes & 
        \vtop{\hbox{\strut Stripmap, Extra Wide Swath(EWS)}\hbox{\strut Interferometric Wide Swath(IWS)}}
        \\ \hline

        Ground resolution & 5 m (Stripmap) - 40 m (EWS) \\ \hline
        Scene size (range) & 1000- 5000 km \\ \hline
        Scene size (azimuth) & 80 km (Stripmap) - 400 km (EWS)  \\ \hline
        Pulse repetition frequency & 1 - 3 kHz \\ \hline
    \end{tabular}
    \caption{Sentinel-1 System Parameters \cite{sentinelRef}}
    \label{tab:sentinel_params}
\end{table}

For more information regarding the Sentinel-1 system the reader is referred to \cite{sentinelmission}.

The images acquired by Sentinel-1 are a monthly interferometric time-series. The methodology and processing chain for the raw processing image data is based on the one developed in \cite{Rodrigo,Paolo}. The preprocessing steps for the generation of the images consist of a coregistration of the images with respect to a common master acquisition, normally selected as the one in the middle of the temporal stack. Afterwards, the backscatter value and coherence were estimated by using a convolutional filter with a window size of 5x19 pixels. Since the resolution of the Sentinel-1 in Interferometric Wide swath mode is of 14 m X 3.7 m, this creates a final image with resolution of approximately 70 m X 70 m. The model used for extracting the temporal correlation is the one described in \cite{Paolo}.

The study area covered is over a region in the Rondônia state in Brazil. The area covers the region between south latitudes 7$\degree$50$'$ and 13$\degree$50$'$, and west longitudes 59$\degree$50$'$ and 67$\degree$10$'$. Since this is an area of huge deforestation focus, the ESA has set up a 6-day repeat pass coverage with Sentinel-1 system. For this study area it was downloaded and processed a set of 12 S-1 time series according to the framework explained in figure \ref{fig:sentinelStack}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Cap2-Methods/sentinelstack.jpg}
    \caption{Sentinel-1 stack of acquisition images. There are four different orbits of acquisitions and each orbit provided 5 images, resulting in 20 total images (of which only 12 were selected). A dot represents a master image (the image that is used as master for the coregistration method) and the other images are slave images.}
    \label{fig:sentinelStack}
\end{figure}{}

In table \ref{tab:sentinelStackTable} it is also possible to see the geographical description of each orbit and image acquired.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c||c|c|c|c|}
        \hline  
        \multicolumn{7}{|c|}{Corner Coordinates} \\ 
        \hline \hline
        \textbf{Stack} & \textbf{Orbit} &\textbf{Name} &\textbf{Lat. Min} &\textbf{Lat. Max} &\textbf{Lon. Min} &\textbf{Lon. Max} \\
        \hline
        1&010&$TS_0$ &9$\degree$40$'$58$"$S &7$\degree$42$'$53$"$S &59$\degree$52$'$18$"$W &61$\degree$44$'$43$"$W \\
        2&010&$TS_1$ &11$\degree$16$'$36$"$S &9$\degree$15$'$31$"$S &60$\degree$12$'$59$"$W &62$\degree$5$'$52$"$W \\
        3&010&$TS_2$ &12$\degree$45$'$21$"$S &10$\degree$43$'$21$"$S &60$\degree$33$'$20$"$W &62$\degree$26$'$54$"$W \\
        4&010&$TS_3$ &9$\degree$40$'$58$"$S &7$\degree$42$'$53$"$S &59$\degree$52$'$18$"$W &61$\degree$44$'$43$"$W \\
        5&054&$TS_0$ &14$\degree$10$'$32$"$S &12$\degree$12$'$43$"$S &60$\degree$53$'$48$"$W &62$\degree$46$'$54$"$W \\
        6*&083&$TS_0$&10$\degree$12$'$15$"$S &8$\degree$5$'$50$"$S &66$\degree$8$'$34$"$W &67$\degree$59$'$40$"$W \\
        7*&083&$TS_1$&10$\degree$22$'$8$"$S &8$\degree$32$'$54$"$S &62$\degree$4$'$44$"$W &63$\degree$37$'$30$"$W \\
        8*&083&$TS_2$&11$\degree$51$'$16$"$S &10$\degree$2$'$26$"$S &62$\degree$25$'$15$"$W &64$\degree$19$'$5$"$W \\
        9*&083&$TS_3$&13$\degree$24$'$3$"$S &11$\degree$32$'$42$"$S &62$\degree$44$'$38$"$W &64$\degree$40$'$34$"$W \\
        10&156&$TS_0$&9$\degree$24$'$34$"$S &8$\degree$4$'$15$"$S &63$\degree$53$'$30$"$W &65$\degree$56$'$2$"$W \\
        11&156&$TS_1$&10$\degree$15$'$7$"$S &8$\degree$48$'$35$"$S &64$\degree$5$'$7$"$W &66$\degree$8$'$22$"$W \\
        21&156&$TS_2$&10$\degree$36$'$21$"$S &9$\degree$46$'$22$"$S &64$\degree$9$'$40$"$W &66$\degree$19$'$6$"$W \\
        \hline
    \end{tabular}
    \caption{Sentinel-1 stack description. The stacks marked with asterisk are images chosen for validation and others are used for the Random Forest Training.}
    \label{tab:sentinelStackTable}
\end{table}

A visual representation of the rondonia dataset can be seen in figure \ref{fig:rondoniadataset}. The images acquisitions are overlapped with a Google Maps image of the Rondônia state. From image \ref{fig:rondoniadataset} there are 3 different pixels color that can be seen: green pixels - which represents forest areas - red pixels - which represent deforested areas - and blue pixels - which represent man-made surfaces. This reference map was created by PRODES (programa de cálculo do desflorestamento da amazônia) project.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Cap2-Methods/rondonia_dataset.jpg}
    \caption{Reference deforestation map for the Rondônia dataset provided by PRODES. The blue pixels represent man-made surfaces, the green pixels represent forest areas, and the red pixels represents deforested areas. The yellow number represents the corresponding acquisition orbit.}
    \label{fig:rondoniadataset}
\end{figure}{}

\subsection{The CARABAS system and dataset}

The CARABAS-II is the second generation SAR system designed by the Swedish Defense Research Agency (FOI) for FOPEN applications.
The CARABAS-II has participated in numerous military campaigns and has been used for research purpose since the nineties.
The radar is a VLF UHB SAR system that transmits HH-polarized radio waves in the frequency range of 
20-90 MHz, therefore having resolution in the range of 3.3-15 m. The radar antenna is mounted on a Sabreliner aircraft as seen in \figref{fig:sabreliner}.

\begin{figure}[H]
    \centering
    \includegraphics{chapter6/sabreliner.jpg}
    \caption{The CARABAS-II VHF SAR mounted in front of a Sabreliner airplane.}
    \label{fig:sabreliner}
\end{figure}

In the table \cite{tab:carabas_system} the system parameters for the CARABAS-II system are presented for the
flight campaigns that were used in this work. 

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        System Parameters & Values \\ \hline \hline
        Nominal flight altitude & 3 - 9 km \\ \hline
        Nominal flight speed & 127 m/s \\ \hline
        Frequency band & 20-86 MHz \\ \hline
        Aperture angle & 90 degrees \\ \hline
        Transmitted power & 500 W \\ \hline
        Pulse modulation & Non-linear frequency modulation \\ \hline
        Radio Frequency Interference (RFI) sniff & On \\ \hline
        Frequency sub-bands & 35 (36 with RFI-sniff) \\ \hline
        Frequency step & 1.875 Mhz \\ \hline
        Center frequencies & 21.25-85Mhz \\ \hline
        Pulse repetition & frequency 5000Hz \\ \hline
        Pulse length & 15$\mu$s \\ \hline
        Maximum range & 26.4 km \\ \hline
    \end{tabular}
    \caption{CARABAS-II SAR system parameters. Source: \cite{76}}
    \label{tab:carabas_system}
\end{table}

By trying to promote research on change detection algorithms for wavelength-resolution
images, FOI created a dataset of images acquired by CARABAS-II and made it publicly available. 
This dataset is the one used to test and assess the quality of the proposed CDA.

The dataset consists of 24 SAR images selected from over 150 images obtained during different flight campaigns.
Each image covers the same ground area of 6 $km^2$ (3 km vertically and 2 km horizontally)
and is given in the form of a 3000 X 2000 matrix, where each pixel size is 1 km x 1 km.
According to \cite{ 77,62,78} the images are already calibrated, pre-processed and geocoded.

The location of the image dataset is inside the military base station Missile Test Area North
Vidsel in northern Sweden in 2002. The test site is a region near the village of Nausta \cite{75}.
The vegetation of the area is dominated by Scots Pine tree \cite{ 76}, which consists of small and medium size trees.
According to \cite{75} the area also contains fields, roads and lakes.

With the objective of testing CDA quality, it was deployed 25 testing targets over the testing with different configuration 
and arrangements. The testing targets consist of ten TGB11 model military vehicles, eight TGB30 model 
military vehicles, and seven TGGB40 model military vehicles. The dimensions of each vehicle are presented in the table \ref{tab:vehicle_dimensions}.
From the table \ref{tab:vehicle_dimensions} it can be seen that the dimensions of the vehicles are similar to the wavelength of the CARABAS-II system,
therefore all advantages of targets with similar dimension to the wavelength previously mentioned hold true for the dataset.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline 
        Military Vehicle & Length & Width & Height & Quantity \\ \hline \hline
        TGB11 & 4.4 m & 1.9 m & 2.2 m & 10 \\ \hline
        TGB30 & 6.8 m & 2.5 m & 3.0 m & 8 \\ \hline
        TGB40 & 7.8 m & 2.5 m & 3.0 m & 7 \\ \hline
    \end{tabular}
    \caption{Target dimensions}
    \label{tab:vehicle_dimensions}
\end{table}


The dataset of 24 images were acquired using four different flight passes. Each 
flight pass has an incidence angle of 58 degrees, used the Strip SAR mode and was acquired with the radar looking left \cite{ 75,76}.
The vehicles were positioned in two different forests, Forest 2 being in the northwest of the field, and forest 1 being in the
southeast of the test area. 

\begin{figure}[H]
    \centering
    \includegraphics{chapter6/carabas_vehicles_fisico.jpg}
    \caption{Military vehicles used as target. (left) TGB11. (middle) TGB30. (right) TGB40. 
    This picture also depicts the vegetation characteristics of the test area.}
    \label{fig:veiculos}
\end{figure}

The vehicles in mission 2 are positioned in forest 2 and have a heading angle of 225 degrees pointing southwest direction;
vehicles in mission 3 are positioned in forest 2 and have a heading angle of 315 degrees pointing northwest direction;
vehicles in mission 4 are located in forest 1 and have the same heading angle of mission 2;
vehicles in mission 5 are located in forest 1 and have a heading angle of 270 degrees pointing west direction.
Vehicles in mission 2 and 3 are separated approximately by 50 m, as such for vehicles in mission 4 and 5.
\figref{fig:carabas_vehicles} presents images of each mission with the vehicles area highlighted in red.

\begin{figure}[H]
    \centering
    \includegraphics{chapter6/carabas_vehicles.jpg}
    \caption{Examples of CARABAS-II images. (a) is mission 2, (b) is mission 3, (c) is mission 4 and (d)
    is mission 5.}
    \label{fig:carabas_vehicles}
\end{figure}

In table \ref{tab:flight_mission} it is presented the summary with the information of each image in the dataset

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline 
        Image Number & Mission & Pass & Flight Heading (degrees) & Target Heading \\ \hline \hline
        1 & 2 & 1 & 225 & 225  \\ \hline
        2 & 2 & 2 & 135 & 225  \\ \hline
        3 & 2 & 3 & 225 & 225  \\ \hline
        4 & 2 & 4 & 135 & 225 \\ \hline
        5 & 2 & 5 & 230 & 225 \\ \hline
        6 & 2 & 6 & 230 & 225 \\ \hline
        7 & 3 & 1 & 225 & 315 \\ \hline
        8 & 3 & 2 & 135 & 315 \\ \hline
        9 & 3 & 3 & 225 & 315 \\ \hline
        10 & 3 & 4 & 135 & 315 \\ \hline
        11 & 3 & 4 & 135 & 315 \\ \hline
        12& 3 & 6 & 230 & 315  \\ \hline
        13 & 4 & 1 & 225 & 225  \\ \hline
        14 & 4 & 2 & 135 & 225  \\ \hline
        15 & 4 & 3 & 225 & 225  \\ \hline
        16 & 4 & 4 & 135 & 225  \\ \hline
        17 & 4 & 5 & 230 & 225  \\ \hline
        18 & 4 & 6 & 230& 225 \\ \hline
        19 & 5 & 1 & 225 & 270  \\ \hline
        20 & 5 & 2 & 135 & 270  \\ \hline
        21 & 5 & 3 & 225 & 270  \\ \hline
        22 & 5 & 4 & 135 & 270  \\ \hline
        23 & 5 & 5 & 230 & 270  \\ \hline
        24 & 5 & 6 & 230 & 270  \\ \hline
    \end{tabular}
    \caption{Measurements parameters for each image}
    \label{tab:flight_mission}
\end{table}


\section{Methods}
\subsection{Texture methods for image analysis}

As previously mentioned, the textures might give useful information for classification because sometimes the neighborhood of a pixel also provides valuable information for the classification. In forest landscapes, for example, the texture value might depend on the size and distance between trees, such that in high-resolution images if two pixels fall in the same tree then they will have similar value, resulting in a small local variance of intensities, something that will be indicated by the texture value. According to \cite{Woodcock} if the resolution is increased to a size comparable to the size to of trees, then the local variance also increases, something noticeable in tropical forests with a high species diversity. Therefore, is important to mention that the texture is dependent of the resolution of the image, and the texture of a high resolution image of an area might be different from a low resolution image of the same area.

On this work it will be used three different textures methods for improving classification results: the Grey level co-occurrence matrix(GLCM) method, the Laws textures method and the sum and different histograms methods.

\subsubsection{The GLCM Method}
\label{sec:GLCM_Method}

The first method of texture creation is the grey level co-occurrence matrix (GLCM) method.
A co-occurrence matrix is a matrix extracted from an image in which the values in the rows and columns of this matrix represent the set of possible grey scale values of the image. 
For example, the co-occurrence matrix $C$ is a matrix in which the elements represent the possible co-occurrence of values for the image $I$ given a spatial relationship on a set of possible values $V$. 
For example, given an image, the co-occurrence matrix $C$ indicates how many times the value $i$ co-occurs with the value $j$ given a spatial relationship. 
This spatial relationship is given by a displacement vector $d = (d_r, d_c)$ that dictates the distance of the pixels that one wants to analyze the co-occurrence of values.
A more mathematical way to express this matrix is given by the following definition:

\begin{equation}
    C_{d}(i,j) = \# \{(r,c) | I(r,c)=i\  \textit{and} \  I(r+d_r, c+d_c)=j \} 
\end{equation}

For example, if the grey scale image $I$ is equal to:

\begin{equation}
I=
    \begin{bmatrix}
    1&1&0&0\\
    1&1&0&0\\
    0&0&2&2\\
    0&0&2&2
    \end{bmatrix} 
\end{equation}{}


Then three co-occurrence matrices for different displacement vectors $d=(0,1)$, $d=(1,0)$ and $d=(1,1)$ are:
\begin{equation}
    C_{(0,1)}=
    \begin{bmatrix}
    4&0&2\\
    2&2&0\\
    0&0&2
    \end{bmatrix}
\end{equation}{}

\begin{equation}
C_{(1,0)}=
    \begin{bmatrix}
    4&0&2\\
    2&2&0\\
    0&0&2
    \end{bmatrix}
\end{equation}{}

\begin{equation}
C_{(1,1)}=
    \begin{bmatrix}
    2&0&2\\
    2&1&1\\
    0&0&1
    \end{bmatrix}
\end{equation}{}

In $C_{(0,1)}$ the position $(0,0)$ equals 4, which means that $j=0$ appears to the right of $i=0$ four times in the image. The position $(1,0)$ equals 2 because $j=0$ appears to the right of $i=1$ two times in the image. The position $(2,1)$ equals 0 because $j=1$ never appears to the right of $i=2$.

From this grey level co-occurrence matrix it is important to generate the \textit{normalized} grey level co-occurrence matrix $N_d$ given by:

\begin{equation}
\label{eq:normalized}
    N_d(i,j) = \frac{C_d(i,j)}{\sum_i\sum_j C_d(i,j)}
\end{equation}
Co-occurrence matrices capture the texture properties, but are not directly useful for further analysis, such as comparing different textures. To solve this problem, is possible to compute numeric features from this normalized co-occurrence matrix that might give useful information of the image that otherwise would be hidden for machine learning classification algorithms. These numeric features computed from the normalized co-occurrence matrix represent more compactly the textures of the image.
The following textures are features that can be obtained from normalized co-occurrence matrix:
\begin{table}[H]
\centering
\begin{tabular}{ |c |c |}
 \hline
 Contrast & $\sum_{i,j=0}^{N-1} i\frac{N_d(i,j)}{1+(i-j)^2}$\\ 
 Dissimilarity & $\sum_{i,j=0}^{N-1} i N_d(i,j)|i-j|$\\
 Homogeneity & $\sum_{i,j=0}^{N-1}\frac{N_d(i,j)}{1+|i-j|}$\\
 Angular\ Second\ Moment & $\sum_{i,j=0}^{N-1} i N_d(i,j)^2$\\
 Correlation & $\sum_{i,j=0}^{N-1}\frac{(i-\mu)(j-\mu)N_d(i,j)}{\sigma^2}$\\
 Energy & $\sum_{i,j=0}^{N-1}N_d(i,j)^2$\\
 Entropy & $ -\sum_{i,j=0}^{N-1}N_d(i,j)\log_2(N_d(i,j))$\\
 \hline
\end{tabular}
\caption{Formula for texture calculations for GLCM Method}
\label{table:glcm_calculations}
\end{table}

Where $\mu$ is the GLCM mean and $\sigma$ is the standard deviations of the GLCM given by:

\begin{equation}
    \mu=\sum_{i,j=0}^{N-1}iN_d(i,j)
\end{equation}
\begin{equation}
    \sigma^2=\sum_{i,j=0}^{N-1}N_d(i,j)(i-\mu)^2
\end{equation}

A question that rises from this method is how to choose the displacement vector $d=(d_r,d_c)$ such that the result is the best. A solution proposed by  \cite{Zucker} is to run a hypothesis test to check the displacement vector $d$ that most rejects the hypothesis that the pair of pixels separated by the displacement vector $d$ are independent, therefore accepting the hypothesis that this pair of pixels give the most information one of the another and are better suited to compute a texture feature.
The hypothesis test suggests running a $\chi^2$ statistical test to find the value $d$ that give the most information, that is, to maximize the value:
\begin{equation}
    \chi^2(d) = (\sum_i\sum_j \frac{N_d^2(i,j)}{\sum_jN_d(i,j)\sum_iN_i(i,j)} -1)
\end{equation}

\subsubsection{The Laws Textures}
\label{sec:Laws_Textures}

A different way to create textures is to use 2D filters to detect different kinds of textures. Laws developed a method to measure the amount of variation of the image on a fixed sized window. The method consists of making 16 different convolutions of the original image with convolution masks and use the result to compute the texture. The 16 different filters are obtained by taking the product of the following vectors.
\begin{align}
L5 (Level) = 
\begin{bmatrix}
1&4&6&4&1
\end{bmatrix}\\
E5 (Edge) = 
\begin{bmatrix}
-1&-2&0&2&1
\end{bmatrix}\\
R5 (Ripple) = 
\begin{bmatrix}
-1&0&2&0&-1
\end{bmatrix}\\
S5 (Spot) = 
\begin{bmatrix}
1&-4&6&-4&1
\end{bmatrix}
\end{align}

Then the 2D convolution masks are obtained by taking the product of these vectors. For example the R5S5 mask is computed as the product of R5 and S5 as following:
\begin{equation}
\begin{bmatrix}
-1\\0\\2\\0\\-1
\end{bmatrix}
\times
\begin{bmatrix}
1&-4&6&-4&1
\end{bmatrix}
=
\begin{bmatrix}
-1&0&2&0&-1\\
4&0&-8&0&4\\
-6&0&12&0&-6\\
4&0&-8&0&4\\
-1&0&2&0&-1
\end{bmatrix}
\end{equation}

After applying the 16 5x5 masks to the image it is necessary to generate the texture energy map on a fixed size window. Let the window size be 15x15 and $F_k[i,j]$ the result of the $kth$ mask on the pixel [i, j]. Then the texture energy map $E_k$ is defined by:
\begin{equation}
E_k[r,c] = \sum_{j=c-7}^{c+7}\sum_{i=r-7}^{r+7} |F_k[i,j]|   
\end{equation}

After creating the 16 energy maps it is needed to group together similar images that perceive the textures in different directions. For example, if R5S5 detects at ripples in the vertical direction, then S5R5 detects ripples in the horizontal direction, so it is desirable to group them together to detect ripple in both directions. The average of these two images measure then the total ripple content. It is possible to group the 16 energy maps in 9 texture images as following:
\begin{align*}
L5E5/E5L5\qquad L5S5/S5L5\\
L5R5/R5L5\qquad\qquad\ \ E5E5\\
E5S5/S5E5\qquad E5R5/R5E5\\
S5S5\qquad\qquad\ \ S5R5/R5S5\\
R5R5
\end{align*}


\subsubsection{The Sum and Difference Histograms Texture}
\label{sec:sum_and_diff_hist_texture}
\paragraph{The Single Value Decomposition}
Let $I[k,l]$ be a discrete image which is the realization of a bi-dimensional stationary process and let $G=\{1, 2,...,N_g\}$ be the set of the $N_g$ quantized grey levels.
Just as in the GLCM method, consider a pair of pixels $y_1$ and $y_2$ separated by a displacement vector $d = (d_1, d_2)$ such that:
\begin{equation}
\begin{cases}
&y_1 = I[k,l]\\
&y_2 = I[k+d_1, l+d_2]
\end{cases}
\end{equation}
The discrete joint probability function of these two pixels is $P(y_1, y_2)$. And the probability of observing a pair of grey level occurrence $i$ and $j$ separated by a displacement vector $(d_1, d_2)$ is given by:
\begin{equation}
    Prob\{y_1=i,\  y_2=j\} = P(i,j,d_1,d_2) = P(i,j)
\end{equation}
which does not depend on the pixel position $[k,l]$.

Pay attention to the fact that an estimate of this joint distribution $P[i,j]$ is the normalized co-occurrence matrix given by \ref{eq:normalized}, such that:
\begin{equation}
    \hat{P}(i,j) = N_d[i,j] \simeq P(i,j)
\end{equation}
\newline

Let us consider the random vector of the two random variables $y_1$ and $y_2$. Let $Y$ be this random vector such that $Y=[y_1, y_2]^T$. Suppose that this random vector $Y$ has a co-variance matrix $C_{yy}$. The goal is to make a linear transformation on this vector such that the new vector has uncorrelated variables. Suppose the linear transformation is given by a matrix $H$ and the new random vector is $Z$ such that:
\begin{equation}
    Z=HY
\end{equation}{}
It is known from \cite{papoulis} that the co-variance matrix $C_{zz}$ of the random variable $Z$ is obtained by the following formula.
\begin{equation}
    C_{zz} = HC_{yy}H^T
\end{equation}
If we make a single value decomposition on the matrix $C_{yy}$ such that $C_{yy} = U\Lambda^2U^T 
=U\Lambda \Lambda U^T$ and we choose the matrix $H$ such that $H = \Lambda^{-1}U^T$ then the co-variance of the random vector $Z$ is given by:
\begin{equation}
    C_{zz} = HC_{yy}H^T = \Lambda^{-1}U^T U\Lambda \Lambda U^T U \Lambda^{-1} = I
\end{equation}{}
Therefore the components of Z are uncorrelated.

Let us model the random vector $Y$ to have similar cross-correlation between the variables, such that:
\begin{equation}
    C_{yy} = \sigma_y^2 
    \begin{bmatrix}
    1&\rho\\
    \rho&1
    \end{bmatrix}
\end{equation}{}
where
\begin{equation}
    \sigma_y^2\ \rho = E\{(y_1-\mu)(y_2-\mu)\} \ and \ \mu = E\{y_1\} = E\{y_2\}
\end{equation}{}
And due to stationarity 
\begin{equation}
    E\{(y_1-\mu)^2\}=E\{(y_2-\mu)^2\} = \sigma_y^2
\end{equation}{}

Let us make then the transformation given by
\begin{equation}
H =\frac{1}{\sqrt{2}} 
\begin{bmatrix}
1&1\\
1&-1
\end{bmatrix}{}
\end{equation}
That results in the new random vector $Z=[z_1, z_2]^T$ that:
\begin{equation}
\begin{cases}
&z_1 =\frac{y_1+y_2}{\sqrt{2}}\\
&z_2 = \frac{y_1-y_2}{\sqrt{2}}
\end{cases}
\end{equation}
Such that the new co-variance matrix for $Z$ is given by:
\begin{equation}
C_{zz}=
    \begin{bmatrix}
    \sigma_y^2 (1+\rho) & 0\\
    0 & \sigma_y^2 (1-\rho)
    \end{bmatrix}{}
\end{equation}{}

\paragraph{Approximation of Probability Density Functions}
Let $\alpha \in N_g$ and $\beta \in N_g$.
If we assume that the variables for $y_1$ and $y_2$ are gaussian, then $z_1$ and $z_2$ are also gaussian and uncorrelated, therefore the joint probability function can be calculated from:

\begin{equation}
P(y_1=\alpha, y_2=\beta) = P(z_1=\alpha + \beta, z_2=\alpha - \beta) = P_s(z_1=\alpha+\beta)\cdot P_d(z_2=\alpha-\beta) 
\end{equation}

If the variables are not gaussian then the equality is not valid, but it is still a valid approximation to the probability function $P(y_1, y_2)$. 
\begin{equation}
    \hat{P}_y(i,j) = c_0\cdot P_s(i+j) \cdot P_d(i-j) \simeq P_y(i,j)
    \label{approximation_gaussian}
\end{equation}{}
Where $c_0$ is a normalization constant that assures that
\begin{equation}
    \sum_{i=1}^{N_g}\sum_{j=1}^{N_g} \hat{P}_y(i,j) = 1
\end{equation}{}

The relative error of this approximation can be computed analyzing the Kullback-Leibler Divergence between the two variables $P$ and $\hat{P}$ as follows
\begin{equation}
\label{eq:divergence}
    I(P, \hat{P}) = \sum_i\sum_j P_y(i,j)\cdot log(\frac{P_y(i,j)}{\hat{P}(i,j)}) = \\
    H_s + H_d - H_y -log(c_0)\geq0
\end{equation}{}
Where $H_s$, $H_d$ and $H_y$ are the entropies given by:
\begin{equation}
\begin{cases}
&H_y = -\sum_{i=1}^{N_g}\sum_{j=1}^{N_g}P_y(i,j)\cdot log(P_y(i,j)\\
&H_s = -\sum_{k=2}^{2N_g}P_s(k)\cdot log(P_s(k)\\
&H_d = -\sum_{l=-N_g+1}^{N_g-1}P_d(l)\cdot log(P_d(l))
\end{cases}
\end{equation}
This divergence is a measure of the error of approximating one distribution for another, and is equal to zero only when the two variables have the same distribution, therefore it will only be zero if $P(i,j)=\hat{P}(i,j)$. The importance of \ref{eq:divergence} is that it is possible to see the independence of the new variables by comparing only the entropy of the sum and difference to the entropy of the co-occurrence matrix. The closer the mutual information $I(P, \hat{P})$ is to zero, the better the approximation defined by \ref{approximation_gaussian}

\paragraph{Textures Features}
It was shown that the sum and difference are a linear transformation that uncorrelates the random variables of the pair of the pixels. Therefore, it is suggested by \cite{Unser} to replace the regular co-occurrence matrix by the associated histograms of the sum and difference estimated from the original image.
The non-normalized sum and difference associated with the displacement vector $d=(d_1,d_2)$ are given by:
\begin{equation}
\begin{cases}
&s_{k,l} = I[k,l] + I[k+d_1, l+d_2]\\
&d_{k,l} = I[k,l] - I[k+d_1, l+d_2]
\end{cases}
\end{equation}

And then we make the normalized histograms of these new vectors. The histograms of the sum and difference are given by:
\begin{equation}
\begin{cases}
&h_s(i, d_1, d_2) = h_s(i) = |\{(k,l) \in G, \ s_{k,l} = i\}|\\
&h_d(j, d_1, d_2) = h_d(j) = |\{(k,l) \in G, \ d_{k,l} = j\}|
\end{cases}
\end{equation}
And the normalized sum and difference histograms are given by:
\begin{equation}
\begin{cases}
&\hat{P}_s(i) = \frac{h_s(i)}{\sum_i h_s(i)} \qquad (i=2,...,2N_g)\\
&\hat{P}_d(i) = \frac{h_d(i)}{\sum_j h_d(i)} \qquad (j=-N_g+1, ... ,N_g-1)
\end{cases}
\end{equation}
And these normalized sum and difference histograms are estimates of the sum and difference probability functions given by:
\begin{equation}
\begin{cases}
&P_s(i) = Prob\  \{s_{k,l} = i\} \qquad (i=2,...,2N_g)\\
&P_d(j) = Prob\  \{d_{k,l} = j\} \qquad (j=-N_g+1, ... ,N_g-1)
\end{cases}
\end{equation}


From these estimates of the probability density functions it is possible to compute textures. There are 9 important textures that will be used through this work. On the table below there are the computation formulas for them:


\begin{table}[H]
\centering
\begin{tabular}{ |c |c |}
 \hline
 mean & $\frac{1}{2}\sum_i i\hat{P}_s(i)$\\ 
 variance & $\frac{1}{2}(\sum_i (i-2\mu)^2\hat{P}_s(i) + \sum_j (j)^2\hat{P}_d(j))$\\
 energy & $\sum_i \hat{P}_s(i)^2 \ \sum_j \hat{P}_d(j)^2$\\
 correlation & $\frac{1}{2}(\sum_i (i-2\mu)^2\hat{P}_s(i) - \sum_j (j)^2\hat{P}_d(j))$\\
 entropy & $-\sum_i\hat{P}_s(i)log_2(\hat{P}_s(i)) - \sum_j\hat{P}_d(j)log_2(\hat{P}_d(j))$\\
 contrast & $\sum_j j^2 \hat{P}_d(j)^2$\\
 homogeneity & $\sum_j\frac{1}{1+j^2} \hat{P}_d(j)$\\
 cluster shade & $\sum_i (i-2\mu)^3 \hat{P}_s(i)$\\
 cluster prominence & $\sum_i (i-2\mu)^4 \hat{P}_s(i)$\\
 \hline
\end{tabular}
\caption{Formula for texture calculations for Sum and Difference Histograms Method}
\label{table:sum_and_diff_calculations}
\end{table}


It is also important to know which size the window should be to compute the histograms for the sum and difference. Through this work the window size chosen was to be a regular window of size 10x10.
The number of gray levels is also important. Through this work, unless specified, the images will be normalized and scaled manually to have 10 different grey levels.
\\
Theoretically, if the variables were gaussians, then this method would give the same result as the Gray-Level Co-Occurrence matrix method, since if the variables are gaussians are uncorrelated then they are independent and the formula given by $(4.29)$ holds exactly and is not just and approximation. If they are not gaussians then the approximation is not perfect, but there is a great computational advantage to use this method, since making histograms of 1-D vectors is much faster than extracting a 2-D co-occurrence matrix. On the next section it will be analyzed if this method gives a nice result compared to the other 2 and if it is worth it to use this method due to the computational advantage.


\subsection{Machine learning methods to image analysis}

Machine Learning methods are currently among the best tools for problem-solving. While there are many types of learning and subdomains, most of them share some characteristics, like minimizing a loss function, the use of data to extract information and have to undergo a training process in order to ``fit'' to the data. These algorithms are also subject to the same underlying problems, which were faced and dealt with in this work, such as over/underfitting, the bias-variance trade off, hyperparameters tunning and regularization.

Machine Learning revolutionized the field of Computer Vision, since before the experts had to handcraft kernels to be used in feature extraction, which often led to a case-by-case scenario, therefore  extremely cumbersome and overengineered. Also, the field had severe problems of performance, the most challenging tasks seemed unreachable using classical methods. Deep Learning specially, offered better results to these problems with the cost of being less interpretable and more computationally expensive. 

In this section, we shall present some of the most modern algorithms used to tackle challenging perceptual problems. Images, due to their high dimensionality, require Machine Learning models that are extremely flexible, in this work we propose the use of Random Forests and Convolutional Neural Networks (CNNs), more specifically a CNN used for segmentation, called U-Net.

\subsubsection{Random Forest}

Random Forests are Machine Learning models build upon simpler models called Decision Trees, which we will briefly discuss in order to better understand the former. 

Decision Trees are basically a graphical representation of if-else rules. In these trees we start at the root and keep choosing our path downwards by comparing the features with the rules at each node, we keep doing this process until we reach a leaf. They are considered a Machine Learning algorithm because the partition rules are \textbf{learned}, the tree is usually built in a top-down approach, where at each node we decide a feature to use as a dividing rule. The chosen feature is decided based on a metric, those metrics usually consist of one of the following: maximizing the accuracy, minimizing the Gini impurity or optimizing the information gain. In Figure \ref{decision} we can see an example of a Decision Tree.

\begin{figure}[H]
    \includegraphics[width=.7\textwidth]{Cap2-Methods/Decision_Tree.jpg}
    \centering
	\caption{Decision Tree applied to the classical Titanic dataset. Image taken from \cite{decisionimg}.}
	\label{decision}
\end{figure}

Decision Trees are notoriously easy to interpret and inexpensive to build, but unfortunately they are very inaccurate \cite{elements}. In order to improve this, we use some well known techniques by the Machine Learning community. We use a technique called \textbf{bagging}, in which we repeatedly select some subset of the available data (with replacement) to fit a Decision Tree, but at each node of the tree we select a random subset of the available features to be used as a partitioning rule. After this process we will have many Decision Trees and the prediction for a new input will be decided by taking the vote of the trees, this built model is called a Random Forest. The illustration of a Random Forest can be seen in Figure \ref{random}. 

\begin{figure}[H]
    \includegraphics[width=\textwidth]{Cap2-Methods/Random_forest_diagram_complete.png}
    \centering
	\caption{A Random Forest being used for classification. Image taken from \cite{randomimg}.}
	\label{random}
\end{figure}

\subsubsection{Convolutional Neural Networks}

Convolutional Neural Networks are a specialized kind of Artificial Neural Network (ANN), in the usual form of the ANN we have each neuron connected to all the neurons in the previous and next layers, as illustrated in the Figure \ref{ann}. This characteristic of an ANN makes the number of connections grow combinatorically large with input/latent dimensions, making it unfeasible to work with images.

\begin{figure}[H]
    \includegraphics[width=8cm]{Cap2-Methods/rede.png}
    \centering
	\caption{Representation of an Artificial Neural Network. Image taken from \cite{neuralimg}.}
	\label{ann}
\end{figure}

In order to solve this problem, in 1988 it was firstly proposed a Convolution Neural Network \cite{cnn_gordo}, where, instead of learning all the connections, we would learn weights of a convolutional kernel, which will be applied locally to the image. Despite the great idea, CNNs did not see much success until the advent of AlexNet \cite{alex}, where CNNs were successfully used for image classification and, since then, have become the go-to norm in perceptual computer vision tasks. 

CNNs work in a sliding window fashion, where each neighborhood contributes for a single pixel in the next layer. This encodes our prior belief that neighbor pixels should influence each other, which is not always true, but greatly reduces the amount of memory needed to process our input making it functional to work with images. The representation of the mechanism of a CNN can be seen in Figure \ref{cnn}.

\begin{figure}[H]
    \includegraphics[width=\textwidth]{Cap2-Methods/image2.png}
    \centering
	\caption{Representation of a Convolutional Neural Network. Image taken from \cite{cnnimg}.}
	\label{cnn}
\end{figure}

Another key technique used with CNNs is the pooling operation, where we take regular spaced grids in the image and condense the spatial information contained in them. The most common form of pooling is the max-pooling, which takes only the maximal value at each grid. This procedure is illustrated in Figure \ref{pooling}.

\begin{figure}[H]
    \includegraphics[width=.7\textwidth]{Cap2-Methods/MaxpoolSample2.png}
    \centering
	\caption{Example of max pooling. Image taken from \cite{poolimg}.}
	\label{pooling}
\end{figure}

This makes the learned representation more translation-invariant \cite{dlbook}, which is greatly desirable in some applications. Pooling is used to aggregate \textbf{semantic} information at the cost of geometric information, since we lose track of the position of the maximal pixel. Another effect of pooling is quickly shrinking the image, which reduces the number of parameters needed for the network.

\subsubsection{U-Net}

The U-Net \cite{unet_gordo} was a network proposed for the task of semantic segmentation, where geometrical information is vital. We saw previously how the image shrinks in the usual pipeline of a CNN, in order to circumvent this problem, the authors used a method called \textbf{transposed convolution}, where we aim to take an input and augment it while applying a learned weight kernel. The authors also used \textbf{skip connections} where we connect one part of the network directly into another (using sums or concatenation). An overview of the network can be seen in Figure \ref{unetimg}.

\begin{figure}[H]
    \includegraphics[width=.7\textwidth]{Cap2-Methods/screenshot_1.png}
    \centering
	\caption{U-Net original architecture. Image taken from \cite{unet_gordo}.}
	\label{unetimg}
\end{figure}

The key takings from this work were the use of transposed convolutions exploiting the strong semantic information acquired by the network linked to the geometrical information that is stored in the downward stream of the network, the combination of these pathways granted U-Net the first place in the EM segmentation challenge. Another important point to the success of this approach is the use of skip connections to improve the backward gradient update. Since the gradient has to move a shorter path to reach the earlier layers, it usually avoids the common vanishing and exploding problems.

U-Net was also used in a small dataset problem (30 images) which is approximately equal to the CARABAS dataset (24 images), proving that the method did not need huge datasets in order to fit properly.